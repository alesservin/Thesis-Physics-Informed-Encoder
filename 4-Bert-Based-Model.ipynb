{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e049d-6b99-4c77-8419-cf9e19ef7ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable deterministic behaviour on cublas according to pytorch error recommendation.\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # or \":16:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "08e1e5a2-c9c9-4782-aa19-22e137ba00b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from IPython.display import Image, display\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f6cc83ae-9abf-47a0-bedd-620b14d19b97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3bf7b00a-b96f-4ade-a25a-b6f2666ccd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset to use\n",
    "# dataset_folder = \"Battery_and_Heating_Data_in_Real_Driving_Cycles-Splits-of-TripB14\"\n",
    "# dataset_folder = \"Battery_and_Heating_Data_in_Real_Driving_Cycles-Splits-in-Microtrips\"\n",
    "dataset_folder = \"Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bfd896e6-e522-4d7c-9f22-b75f3aaf3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_physics = False\n",
    "use_ml = True\n",
    "HYBRYD_MODEL = False    # True -> one output from model, False -> 2 outputs [Out_ML, Out_Physics]\n",
    "lambda_ = 2\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f6aa47c7-a4f6-4035-8635-2c4b008361bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = [\n",
    "    \"Time [s]\", \"SoC_0\", \"Q_rated\", \"Battery Voltage [V]\", \n",
    "    \"Battery Current [A]\", \n",
    "    \"Battery Temperature [Â°C]\", \n",
    "    \"Time_difference\", \n",
    "    \"SoC [%]\"\n",
    "]    # always the target variable should be the last element of this list\n",
    "\n",
    "calculated_columns = [\n",
    "    # \"SoC_0\", \n",
    "    \"Q_rated\", \n",
    "    \"Time_difference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "99fc9d96-d149-4e3e-91b0-7d8133ee4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE_DATA = True\n",
    "BATCH_SIZE = 1024 * 40    # 1024 * 40 or 6\n",
    "ML_MODEL_USES_CALCULATED_COLUMNS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa883d-9ad2-4d89-bb06-d3e488d1a2b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126d3ca0-736f-4d62-bc98-94c2442e4555",
   "metadata": {},
   "source": [
    "## Define columns to use in each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67306bd9-5252-40ac-aed4-bd73fcbdf926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SoC_0': 1, 'Battery Current [A]': 4, 'Time_difference': 6, 'Q_rated': 2}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get indexes of features that are relevant for the physics model\n",
    "relevant_features_for_physics_model = [\"SoC_0\", \"Battery Current [A]\", \"Time_difference\", \"Q_rated\"]\n",
    "\n",
    "feature_ids_relevant_for_physics_model = {}\n",
    "for feature in relevant_features_for_physics_model:\n",
    "    feature_ids_relevant_for_physics_model[feature] = required_columns.index(feature)\n",
    "\n",
    "feature_ids_relevant_for_physics_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "16697ebe-c9f3-42d6-b3e6-eba6bcdcc0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 4, 5]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define which features will be used as input for ML model (original from dataset + calculated or only original from dataset)\n",
    "\n",
    "# get indexes of calculated_columns, and \n",
    "all_columns_indexes = list(range(len(required_columns)-1))    # all indexes. The -1 is necessary to exclude the last column, the target variable.\n",
    "indexed_calculated_columns = [idx for idx, column_name in enumerate(required_columns) if column_name in calculated_columns]\n",
    "indexed_calculated_columns\n",
    "\n",
    "# according to setting, decide if all columns will be used, or only the non-calculated ones\n",
    "if ML_MODEL_USES_CALCULATED_COLUMNS: \n",
    "    features_indexes_for_ml = all_columns_indexes\n",
    "else:\n",
    "    features_indexes_for_ml = [i for i in all_columns_indexes if i not in indexed_calculated_columns]\n",
    "    \n",
    "features_indexes_for_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5634af6b-d702-44e0-8e49-12232e92eaec",
   "metadata": {},
   "source": [
    "## Load files as dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2478b9f2-af64-4938-abcd-743432b56b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths_with_extension(file_extension=\".csv\", directory=\".\"):\n",
    "    output = []\n",
    "    \n",
    "    for file in os.listdir(directory):  \n",
    "        # Check if the file has the required extension\n",
    "        if file.endswith(file_extension):\n",
    "            output.append(f\"{directory}/{file}\") \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7aad038-a015-49ad-86f5-61b2ed0a7264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/train/TripA01.csv',\n",
       " './data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/train/TripA02.csv',\n",
       " './data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/train/TripA03.csv',\n",
       " './data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/train/TripA04.csv',\n",
       " './data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/train/TripA05.csv']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get filepaths of all train micro-trips\n",
    "train_trips_filepaths = get_filepaths_with_extension(file_extension=\".csv\", directory=f\"./data/{dataset_folder}/train\")\n",
    "train_trips_filepaths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3269a552-dd76-47bd-a467-f14b7267c150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/dev/TripB19_part1.csv',\n",
       " './data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/dev/TripB20_part1.csv',\n",
       " './data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/dev/TripB21_part1.csv',\n",
       " './data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/dev/TripB22_part1.csv',\n",
       " './data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/dev/TripB23_part1.csv']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get filepaths of all dev micro-trips\n",
    "val_trips_filepaths = get_filepaths_with_extension(file_extension=\".csv\", directory=f\"./data/{dataset_folder}/dev\")\n",
    "val_trips_filepaths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "820154f9-5fbb-4b39-8fb4-b52aaa94f0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/test/TripB29_part1.csv',\n",
       " './data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/test/TripB30_part1.csv',\n",
       " './data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/test/TripB31_part1.csv',\n",
       " './data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/test/TripB32_part1.csv',\n",
       " './data/Battery_and_Heating_Data_in_Real_Driving_Cycles-Full_trips/test/TripB33.csv']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get filepaths of all test micro-trips\n",
    "test_trips_filepaths = get_filepaths_with_extension(file_extension=\".csv\", directory=f\"./data/{dataset_folder}/test\")\n",
    "test_trips_filepaths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "450d65b3-4876-4fc5-8f92-2fecbd29fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653b747-1042-4d18-bd01-7327c75c3991",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_train_dfs = [pd.read_csv(filepath, sep=\";\", encoding=\"ISO-8859-2\") for filepath in train_trips_filepaths]\n",
    "lst_train_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7614ae3-653c-4455-a935-5692b93a47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_val_dfs = [pd.read_csv(filepath, sep=\";\", encoding=\"ISO-8859-2\") for filepath in val_trips_filepaths]\n",
    "lst_val_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344e491-d118-4dbc-b7c8-6891e4c5ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_test_dfs = [pd.read_csv(filepath, sep=\";\", encoding=\"ISO-8859-2\") for filepath in test_trips_filepaths]\n",
    "lst_test_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b34cd1-ad48-494a-b73f-633ad6361154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "810356ff-e676-45b7-9831-5b1d0635918f",
   "metadata": {},
   "source": [
    "## Add relevant calculated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff3161-b824-47f4-be43-0bae648c140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_calculated_columns(df):\n",
    "    battery_voltage=360\n",
    "    battery_usable_capacity_kWh=18.8\n",
    "\n",
    "    # Add initial SoC\n",
    "    df[\"SoC_0\"] = df[\"SoC [%]\"][0]\n",
    "\n",
    "    # Convert capacity to Ampere-seconds (As) using battery voltage\n",
    "    Q_rated = (battery_usable_capacity_kWh * 1000) / battery_voltage * 3600   # Convert kWh to As\n",
    "\n",
    "    # Add q_rated\n",
    "    df[\"Q_rated\"] = Q_rated\n",
    "\n",
    "    # Time difference between samples\n",
    "    time_difference = df['Time [s]'].diff().fillna(0)  # First diff is NaN, set to 0\n",
    "    df[\"Time_difference\"] = time_difference\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11b097-5f1c-4617-b059-979e61c808e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_train_dfs = [add_calculated_columns(df) for df in lst_train_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e6829-4d28-40f6-aa52-9486bff7e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_val_dfs = [add_calculated_columns(df) for df in lst_val_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b2bbf-aec9-4ab4-96fb-5217b65308bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_test_dfs = [add_calculated_columns(df) for df in lst_test_dfs]\n",
    "lst_test_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e9a2a-1b93-43b1-8000-67fc542a40a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26075f89-2c32-4ed0-9e4a-003e9b411009",
   "metadata": {},
   "source": [
    "## Restart Time column to 0 sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff04b97-6e1d-4d3e-9189-aa77f5a4f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restart_time_from_0(df):\n",
    "    df[\"Time [s]\"] = df[\"Time [s]\"] - df[\"Time [s]\"][0]\n",
    "    return df\n",
    "\n",
    "lst_train_dfs = [restart_time_from_0(df) for df in lst_train_dfs]\n",
    "lst_val_dfs = [restart_time_from_0(df) for df in lst_val_dfs]\n",
    "lst_test_dfs = [restart_time_from_0(df) for df in lst_test_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9882dd-0b67-4310-af68-b3a3f3e8147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_train_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7583ea79-d253-43a9-a97e-ea3447b5471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_val_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6461db-9b80-4b2f-9485-37bfa04686cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_test_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195625a6-cf3e-418a-9e29-dc50e8d21e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a7c1b03-265e-403e-b008-610d9689273f",
   "metadata": {},
   "source": [
    "## Remove irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f89186-b403-462d-8750-8c48db1dcf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_irrelevant_columns(df):\n",
    "    return df[required_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7999032-7826-40b0-9a70-1f42bbdc71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_train_dfs = [remove_irrelevant_columns(df) for df in lst_train_dfs]\n",
    "lst_train_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c66d5-feb5-40c0-bb56-b21132b40bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_val_dfs = [remove_irrelevant_columns(df) for df in lst_val_dfs]\n",
    "lst_val_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9e2f3-2f96-438e-90b7-83a9c6117bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_test_dfs = [remove_irrelevant_columns(df) for df in lst_test_dfs]\n",
    "lst_test_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bcd794-9d63-4c4f-9678-b19662bc3fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39eb3d33-2828-4e19-bd79-618ae6ad43fe",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "Help for data preparation from ChatGPT: https://chatgpt.com/share/678e1508-2250-800b-b898-922a65ba0061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01977478-8651-4de0-a694-0c1192506a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae3c5cf-cc72-4b8f-a8c6-4a744485b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_train_dfs[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d740feb-9609-44a0-9a6e-9bfb458cbf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate X and y from dataframe\n",
    "def separate_x_and_y(df):\n",
    "    \"\"\"\n",
    "    It outputs two dataframes, the first one contains only the X values, the second only the y values.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_X = df[required_columns[:-1]]\n",
    "    df_y = df[[\"SoC [%]\"]]\n",
    "    return df_X, df_y\n",
    "\n",
    "# separate x and y in dataframes of all sets\n",
    "lst_train_data = [separate_x_and_y(df) for df in lst_train_dfs]\n",
    "lst_val_data = [separate_x_and_y(df) for df in lst_val_dfs]\n",
    "lst_test_dfs = [separate_x_and_y(df) for df in lst_test_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22f86f-cc6b-4845-981b-21d5cabeaa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_train_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3500fae-5db9-4243-9dc6-0881bca7d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save column names before parsing to numpy arrays\n",
    "X_column_names = lst_train_data[0][0].columns\n",
    "X_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ffc80-6aa1-4d53-8d04-91594adf2405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse data to numpy arrays\n",
    "def parse_dataframes_to_numpy_arrays(dataframes):\n",
    "    # id 0 contains dataframe with X values, id 1 contains dataframe with y values.\n",
    "    return dataframes[0].to_numpy(), dataframes[1].to_numpy()\n",
    "\n",
    "# parse dataframes of all sets\n",
    "lst_train_data = [parse_dataframes_to_numpy_arrays(dataframes) for dataframes in lst_train_data]\n",
    "lst_val_data = [parse_dataframes_to_numpy_arrays(dataframes) for dataframes in lst_val_data]\n",
    "lst_test_dfs = [parse_dataframes_to_numpy_arrays(dataframes) for dataframes in lst_test_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2fa210-e507-4091-9663-9568487f70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_train_data[0][0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7559ec-0a16-4997-8c14-b828d22786b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ = 0\n",
    "for index in range(10):\n",
    "    sum_ += len(lst_test_dfs[index][0])\n",
    "sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e0b01-0f64-4797-a20f-7494e2e4b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c001e-662d-4047-a063-bcb225962a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pseudo-scaler, to use it when NORMALIZE_DATA is false.\n",
    "class IdentityScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Mimics the fit method but does nothing.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Returns the input unchanged.\"\"\"\n",
    "        return np.asarray(X)  # Ensure it's a NumPy array like StandardScaler\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Returns the input unchanged.\"\"\"\n",
    "        return np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1f086-e3fb-47f0-973e-24408c4b978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Scalers, fit them and use them\n",
    "\n",
    "scaler_X = None\n",
    "scaler_y = None\n",
    "\n",
    "# if NORMALIZE_DATA is true, then create and train data scalers, else, scalers should be just identity functions.\n",
    "if NORMALIZE_DATA:\n",
    "    # Initialize scalers for features and target\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "else:\n",
    "    scaler_X = IdentityScaler()\n",
    "    scaler_y = IdentityScaler()\n",
    "\n",
    "# Fit scalers on the training data only\n",
    "train_features = [dataframes[0] for dataframes in lst_train_data]\n",
    "train_target = [dataframes[1] for dataframes in lst_train_data]\n",
    "\n",
    "# Combine training data to fit scalers\n",
    "X_combined = np.vstack(train_features)\n",
    "y_combined = np.vstack(train_target)\n",
    "scaler_X.fit(X_combined)\n",
    "scaler_y.fit(y_combined)\n",
    "\n",
    "\n",
    "def normalize_data(X, y):\n",
    "    X = scaler_X.transform(X)\n",
    "    y = scaler_y.transform(y) \n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Normalize training, validation, and test sets\n",
    "lst_train_data = [normalize_data(data[0], data[1]) for data in lst_train_data]\n",
    "lst_val_data = [normalize_data(data[0], data[1]) for data in lst_val_data]\n",
    "lst_test_dfs = [normalize_data(data[0], data[1]) for data in lst_test_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38607a7a-9fb9-4dfc-abee-12be6eb8a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_train_data[0][0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94355e3-7294-43fd-8266-f6482128ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaler_X.inverse_transform(lst_train_data[0][0][0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46474d-0a15-4ca1-957f-1b60e62ab1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(list_datasets, batch_size=32):\n",
    "    # function to convert data to DataLoader. Each dataloader corresponds to one time-series file. \n",
    "    # Since this function returns a list of dataloaders, it can be seen as returning a list of time-series files, whose data is split in batches.\n",
    "    \n",
    "    dataloaders = []\n",
    "    for X_batch, y_batch in list_datasets:\n",
    "        X_tensor = torch.tensor(X_batch, dtype=torch.float32).to(device)\n",
    "        y_tensor = torch.tensor(y_batch, dtype=torch.float32).to(device)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloaders.append(DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False))\n",
    "    return dataloaders\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loaders = create_dataloader(lst_train_data,BATCH_SIZE)\n",
    "val_loaders = create_dataloader(lst_val_data, BATCH_SIZE)\n",
    "test_loaders = create_dataloader(lst_test_dfs, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ef418-f987-4c02-adcc-a31ba5852d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if batch size is less or equal the BATCH_SIZE that has been set\n",
    "for x, y in train_loaders[0]:\n",
    "    print(f\"Batch size from index 0 in dataloader: {len(x)}\")\n",
    "    break\n",
    "\n",
    "print(f\"BATCH_SIZE: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eb164d-9c97-4eaf-915f-b8889e62ee89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"X values: \", lst_train_data[0][0], \"Y values\", lst_train_data[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104bf4b5-d568-4ccc-bae3-90e25c565f64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80957c-2fb9-466a-bcb1-982b170e87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Replace the word embeddings layer with an identity layer.\n",
    "# Now, model.bert.embeddings.word_embeddings will just pass through its input.\n",
    "bert.embeddings.word_embeddings = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e8d14-067c-4797-a788-24d9174f544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd36926-355a-4c2b-8e17-3815cec28a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    # base architecture from https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation/notebook\n",
    "    # code modification based on AI Assitant: https://chatgpt.com/share/67bdc88e-1974-800b-a014-0412fd8416c4\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        \n",
    "        self.bert = bert \n",
    "        \n",
    "        # dropout layer\n",
    "        # self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,1)\n",
    "\n",
    "        #softmax activation function\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    \n",
    "    #define the forward pass\n",
    "    # def forward(self, sent_id, mask):\n",
    "        \n",
    "    #     # #pass the inputs to the model  \n",
    "        # _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        # x = self.fc1(cls_hs)\n",
    "\n",
    "        # x = self.relu(x)\n",
    "\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # # output layer\n",
    "        # x = self.fc2(x)\n",
    "      \n",
    "        # # apply softmax activation\n",
    "        # # x = self.softmax(x)\n",
    "\n",
    "        # return x\n",
    "\n",
    "    def forward(self, sent_embeds, mask):\n",
    "        # Pass the precomputed embeddings to BERT using the inputs_embeds argument\n",
    "        _, cls_hs = self.bert(inputs_embeds=sent_embeds, attention_mask=mask, return_dict=False)\n",
    "        \n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        # x = self.dropout(x)  # Ensure self.dropout is defined in __init__\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7eeb4-f717-4a25-a039-6bb01ba1917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer code based on https://chatgpt.com/share/6797a3c0-85b8-800b-92bd-45a43f1af080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b14d510-64e8-4af8-bf8c-831082e477c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help from AI assistant https://chatgpt.com/share/6797bf62-3674-800b-84c6-3b455e40aa44\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # based on https://stackoverflow.com/questions/77444485/using-positional-encoding-in-pytorch\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000):  # max_len can be increased if necessary\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = self.pe.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)  # Current sequence length\n",
    "        if seq_len > self.pe.size(1):\n",
    "            raise ValueError(f\"Input sequence length {seq_len} exceeds max_len {self.pe.size(1)}.\")\n",
    "        return x + self.pe[:, :seq_len, :].to(x.device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe172d1f-c2f3-4204-b78c-b0f6f58d985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoulombCounting(nn.Module):\n",
    "    # help from AI assistant: https://chatgpt.com/share/679931c6-a3b8-800b-870e-0a5eae035398\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CoulombCounting, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_soc(data):\n",
    "        \"\"\"\n",
    "        Calculate the State of Charge (SoC) using the Coulomb Counting method.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract columns data\n",
    "        # TO DO: add logic to indicate which indexes contains which datafields. e.g.: SoC_0 has index 1, Battery_current has id 4, and so on.\n",
    "        # print(f'battery_current: {data[:, feature_ids_relevant_for_physics_model[\"Battery Current [A]\"]]}')\n",
    "        soc_0 = data[:, feature_ids_relevant_for_physics_model[\"SoC_0\"]]\n",
    "        battery_current = data[:, feature_ids_relevant_for_physics_model[\"Battery Current [A]\"]]\n",
    "        time_difference = data[:, feature_ids_relevant_for_physics_model[\"Time_difference\"]]\n",
    "        q_rated = data[:, feature_ids_relevant_for_physics_model[\"Q_rated\"]]\n",
    "    \n",
    "        # Coulomb Counting Method to estimate SOC\n",
    "        estimated_soc = soc_0 + (battery_current * time_difference).cumsum() / q_rated * 100\n",
    "        \n",
    "        return estimated_soc\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.calculate_soc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a82991-a90a-4187-8ee6-84559f653366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: get only data from columns that are not calculated\n",
    "for train_loader in train_loaders:\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        print(X_batch[:, features_indexes_for_ml])\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b5360-78ff-41f4-bef8-620d1cb7ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Physics Informed model\n",
    "\n",
    "# code modification based on AI Assitant: https://chatgpt.com/share/67bdc88e-1974-800b-a014-0412fd8416c4\n",
    "\n",
    "# # Model parameters\n",
    "d_model = 768    # 512 or 32 # Embedding dimension\n",
    "# num_heads = 8    # 8 or 2\n",
    "# num_layers = 6    # 6 or 2\n",
    "# dropout = 0.1\n",
    "# dim_feedforward = 2048    # 2048 or d_model * 4\n",
    "input_dim = len(features_indexes_for_ml)    # according to the number of columns available for the ML model\n",
    "# output_dim = 1\n",
    "\n",
    "# Define your embedding layer (assuming input features)\n",
    "embedding_layer = nn.Linear(input_dim, d_model)\n",
    "\n",
    "# # Instantiate the Positional Encoding\n",
    "# positional_encoding = PositionalEncoding(d_model, max_len=BATCH_SIZE)\n",
    "\n",
    "# # Define Transformer Encoder layer\n",
    "# encoder_layer = nn.TransformerEncoderLayer(\n",
    "#     d_model=d_model, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout\n",
    "# )\n",
    "\n",
    "bert_model = BERT_Arch(bert)\n",
    "\n",
    "coulomb_counter = CoulombCounting()\n",
    "\n",
    "class PhysicsformerModel(nn.Module):\n",
    "    # help from AI assistant: https://chatgpt.com/share/679931c6-a3b8-800b-870e-0a5eae035398\n",
    "    \n",
    "    def __init__(self, use_physics=True, use_ml=True):\n",
    "        super(PhysicsformerModel, self).__init__()\n",
    "        self.use_physics = use_physics\n",
    "        self.use_ml = use_ml\n",
    "\n",
    "        if use_physics == False and use_ml==False:\n",
    "            raise Exception (\"At least one of these paramters should be true: use_physics, use_ml. They can not be False at the same time. \")\n",
    "\n",
    "        # add physical layer, if required\n",
    "        if self.use_physics:\n",
    "            self.physics_layer = coulomb_counter\n",
    "\n",
    "        # add ML layers, if required\n",
    "        if self.use_ml:\n",
    "            self.embedding = embedding_layer\n",
    "            # self.positional_encoding = positional_encoding\n",
    "            # self.transformer_encoder = transformer_encoder\n",
    "            # self.fc_out = nn.Linear(d_model, output_dim)\n",
    "            self.bert = bert_model\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x_out = None\n",
    "        \n",
    "        # Physical part\n",
    "        if self.use_physics:\n",
    "            x_physics = scaler_X.inverse_transform(x.cpu().numpy())    # undo scalling\n",
    "            x_physics = self.physics_layer(x_physics)\n",
    "            x_physics = x_physics.reshape(-1, 1)    # reshape it for the scaler\n",
    "            x_physics = scaler_y.transform(x_physics)    # scale output from physics layer\n",
    "            x_physics = torch.tensor(x_physics).unsqueeze(0)   # add extra dimension at the begining to match output format of ML model (see x_ml)\n",
    "            x_physics = x_physics.to(device)    # parse it to pyTorch tensor\n",
    "\n",
    "        # ML part\n",
    "        if self.use_ml:\n",
    "            # After applying your embedding layer, x_ml has shape (batch_size, 768)\n",
    "            x_ml = self.embedding(x[:, features_indexes_for_ml])  \n",
    "            \n",
    "            # Unsqueeze to add a sequence dimension: now shape is (batch_size, 1, 768)\n",
    "            x_ml = x_ml.unsqueeze(1)\n",
    "            \n",
    "            # Create an attention mask for the single token per sample\n",
    "            mask = torch.ones(x_ml.size()[:2], dtype=torch.long, device=x_ml.device)  # shape: (batch_size, 1)\n",
    "            \n",
    "            # Pass x_ml and mask to the BERT part\n",
    "            x_ml = self.bert(x_ml, mask)\n",
    "\n",
    "        \n",
    "        # prepare model output\n",
    "        if self.use_physics and self.use_ml:\n",
    "            if HYBRYD_MODEL:\n",
    "                x_out = x_physics + x_ml    # sum physics output and ML output\n",
    "            else: \n",
    "                soc_ml = x_ml[:, :, 0:1]  # First column \n",
    "                # lambda_1 = x_ml[:, :, 1:2]  # Second column \n",
    "                # lambda_2 = x_ml[:, :, 2:3]  # Third column \n",
    "                x_out = torch.cat([soc_ml, x_physics])\n",
    "                \n",
    "        elif self.use_physics:\n",
    "            x_out = x_physics    # output is only physics output\n",
    "        elif self.use_ml:\n",
    "            x_out = x_ml    # # output is only ML output\n",
    "        \n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a96d6c-5a0e-47e2-bf25-9f06ec612702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "\n",
    "# make model more determistic\n",
    "# see: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "torch.manual_seed(2025)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(2025)\n",
    "\n",
    "model = PhysicsformerModel(use_physics=use_physics, use_ml=use_ml)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077ad56-9869-4f9c-a7b8-78d72e9ae900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change device of model\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d35b21b-6ed2-483b-b16e-42e707c9ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch optimizer\n",
    "# learning_rate = 1e-3\n",
    "if model.use_ml:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43afb376-4bd3-437d-bda9-5c4201f8bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def rmse_loss(y_true, y_pred):\n",
    "    mse_loss = torch.nn.functional.mse_loss(y_pred, y_true) \n",
    "    return torch.sqrt(mse_loss)  \n",
    "    \n",
    "# loss_function =  torch.nn.MSELoss()  \n",
    "if HYBRYD_MODEL:\n",
    "    loss_function = rmse_loss \n",
    "else:\n",
    "    loss_function = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332a1ea-0ac9-4f8e-bfa5-c13e2c8d1891",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b8dad0-a1f7-4f57-9a16-e094e21055cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_epochs = 500    # 100, 300, 500\n",
    "# patience = 10\n",
    "patience = n_epochs    # patience = n_epochs deactivates early stopping\n",
    "evaluattion_frequency_in_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7f504-cbed-4e37-ae56-8c80e9f31040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count total number of train batches\n",
    "n_train_batches = sum([len(data_loader) for data_loader in train_loaders])\n",
    "n_train_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca187fd-250a-484f-a9bb-e984bab94e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count total number of dev batches\n",
    "n_val_batches = sum([len(data_loader) for data_loader in val_loaders])\n",
    "n_val_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c46279-b3b4-44ec-91bf-fc9e2c4ab084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count total number of test batches\n",
    "n_test_batches = sum([len(data_loader) for data_loader in test_loaders])\n",
    "n_test_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba900b9a-82a7-4c79-bc63-8752b2f23a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory to store results\n",
    "model_name = f\"\"\"{'Physics' if model.use_physics else ''}{'-and-' if model.use_physics and model.use_ml else ''}{'Transformer' if model.use_ml else ''}\"\"\"\n",
    "model_name\n",
    "\n",
    "# create folder to store results, if it does not exist yet\n",
    "params = \"Data Loss + Physics Loss\" if not HYBRYD_MODEL else \"Delta approach\"\n",
    "params += f\"{', lambda='+ str(lambda_) if not HYBRYD_MODEL else ''}\"   # add lambda_ if using model that is not hybrid\n",
    "params += f\", Lr={learning_rate}, epochs={n_epochs}, dropout={dropout}\"\n",
    "params = params.replace(f\"\\n\", \"_\")  # Replace newline with underscore\n",
    "params = re.sub(r'[<>:\"/\\\\|?*\\n]', '_', params)  # Replace invalid characters\n",
    "output_folder = f\"results/{params}/{model_name}\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936f58b-c646-458e-95e6-d03e808d2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694379b-9676-48d5-aac1-67bb1890bbfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf') \n",
    "best_model_state = copy.deepcopy(model.state_dict())\n",
    "patience_counter = 0\n",
    "best_model_epoch = 0\n",
    "\n",
    "training_start_time = time.time()\n",
    "all_training_losses = []\n",
    "all_validation_losses = []\n",
    "all_validation_rmses = []\n",
    "\n",
    "print(f\"HYBRYD_MODEL: {HYBRYD_MODEL}\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss_train = 0.0\n",
    "    print(f\"Epoch no.: {epoch}\")\n",
    "\n",
    "    # train if model contains ML\n",
    "    if model.use_ml:\n",
    "        model.train()\n",
    "        for train_loader in train_loaders:\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                # y_batch = y_batch.unsqueeze(0)  # Add batch dimension, to match the dimensionality of the model output.\n",
    "                \n",
    "                # Forward pass\n",
    "                if HYBRYD_MODEL or (use_physics ^ use_ml):\n",
    "                    predictions = model.forward(X_batch)\n",
    "                    loss = loss_function(predictions, y_batch)\n",
    "                else:\n",
    "                    predictions = model.forward(X_batch)\n",
    "                    prediction_ml = predictions[0]\n",
    "                    prediction_ml = prediction_ml.unsqueeze(0)    # Add 1 dimension\n",
    "                    prediction_physics = predictions[1]\n",
    "                    prediction_physics = prediction_physics.unsqueeze(0)    # Add 1 dimension\n",
    "                    loss = (loss_function(prediction_ml, y_batch)) + \\\n",
    "                            (lambda_ * loss_function(prediction_physics, prediction_ml))    \n",
    "                    \n",
    "        \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "                epoch_loss_train += loss.item()\n",
    "    \n",
    "        all_training_losses.append(epoch_loss_train/n_train_batches)\n",
    "    else:\n",
    "        print(\"Physical part only mode: No training performed.\")\n",
    "    \n",
    "    # Validation\n",
    "    if epoch % evaluattion_frequency_in_epochs == 0:\n",
    "        val_loss = 0.0\n",
    "        val_rmse = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_loader in val_loaders:\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    # y_batch = y_batch.unsqueeze(0)  # Add batch dimension, to match the dimensionality of the model output.\n",
    "                    \n",
    "                    # predictions = model(X_batch)\n",
    "                    # val_loss += loss_function(predictions, y_batch).item()\n",
    "\n",
    "                    if HYBRYD_MODEL or (use_physics ^ use_ml):\n",
    "                        predictions = model.forward(X_batch)\n",
    "                        val_loss += loss_function(predictions, y_batch).item()\n",
    "                        val_rmse += rmse_loss(prediction_ml, y_batch).item()\n",
    "                    else:\n",
    "                        predictions = model.forward(X_batch)\n",
    "                        prediction_ml = predictions[0]\n",
    "                        prediction_ml = prediction_ml.unsqueeze(0)    # Add 1 dimension\n",
    "                        prediction_physics = predictions[1]\n",
    "                        prediction_physics = prediction_physics.unsqueeze(0)    # Add 1 dimension\n",
    "                        val_loss += ( loss_function(prediction_ml, y_batch) + \\\n",
    "                            (lambda_ * loss_function(prediction_physics, prediction_ml)) ).item()\n",
    "                        val_rmse += rmse_loss(prediction_ml, y_batch).item()\n",
    "                    \n",
    "            avg_val_loss = val_loss / n_val_batches\n",
    "            avg_val_rmse = val_rmse / n_val_batches\n",
    "            all_validation_losses.append(avg_val_loss)\n",
    "            all_validation_rmses.append(avg_val_rmse)\n",
    "            print(f\"Training Loss: {epoch_loss_train/n_train_batches:.6f}, Validation Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "    # Early stopping verification\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0  \n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        best_model_epoch = epoch\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break    \n",
    "\n",
    "training_end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b2111-8f7a-4925-8ad6-5f22ef94b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training time in seconds\n",
    "execution_time = training_end_time - training_start_time\n",
    "\n",
    "# Convert execution time to hours and minutes\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "\n",
    "print(f\"Code execution time: {hours} hour{'s' if hours != 1 else ''}, {minutes} minute{'s' if minutes != 1 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c2e27-5d4b-4e19-a50c-a3b96537ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model with minimum evaluation loss\n",
    "torch.save(best_model_state, f\"{output_folder}/model-{model_name}-minimum_eval_loss.pth\")\n",
    "\n",
    "# Save scalers\n",
    "joblib.dump(scaler_X, f\"{output_folder}/X-scaler.pkl\")\n",
    "joblib.dump(scaler_y, f\"{output_folder}/y-scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0fd61-733a-4d63-a132-2aac93f34f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model saved was from epoch no. {best_model_epoch}. Best Validation loss: {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131da51-f823-4fa1-a463-3d34f46a5f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630130ea-8d4a-4e45-9402-72a6e3decc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7a286-c4e9-4f28-a2c4-17a3cdbc3bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # validation losses to cpu\n",
    "# all_validation_losses = [loss.cpu().numpy() for loss in all_validation_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5086230b-ae83-4189-81c4-4eb418535a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses, if the evaluation frequency is 1\n",
    "if evaluattion_frequency_in_epochs == 1:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(0, len(all_training_losses)), all_training_losses, label='Training Loss', linestyle='-')\n",
    "    plt.plot(range(0, len(all_validation_losses)), all_validation_losses, label='Validation Loss', linestyle='--')\n",
    "    \n",
    "    plt.title('Training and Validation Loss Over Epochs', fontsize=16)\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(alpha=0.5)\n",
    "    # display 10 epochs or all (if len(all_validation_losses)//10) < 0) in the xticks\n",
    "    plt.xticks(range(0, len(all_validation_losses), max(1, len(all_validation_losses)//10 )), rotation=45)  \n",
    "\n",
    "    # Save the plot\n",
    "    output_file = f\"{output_folder}/plot-train_vs_evaluation_loss-{model_name}.png\"\n",
    "    plt.savefig(output_file, bbox_inches='tight', dpi=300)  # Save the plot with tight layout and high resolution\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28472bf3-9f47-4b82-b8e4-d07640d7e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cache\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72475bc5-6c6e-43eb-a782-ac616517646b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "# load best model\n",
    "model = None\n",
    "model = PhysicsformerModel(use_physics=use_physics, use_ml=use_ml)\n",
    "\n",
    "model.load_state_dict(torch.load(f\"{output_folder}/model-{model_name}-minimum_eval_loss-epoch-{best_model_epoch}.pth\"))  \n",
    "# model = torch.load(selected_model_location, map_location=device)\n",
    "print(f\"Loaded model\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# define additional metrics\n",
    "mse_loss_function = torch.nn.functional.mse_loss\n",
    "mae_loss_function = torch.nn.functional.l1_loss\n",
    "\n",
    "# get also the prediction for the data in each dataloader. Remember, each dataloader has batched data from one file of the dataset.\n",
    "lst_all_loader_predictions = []    # each element contains predictions for 1 file of the dataset.\n",
    "test_loss = 0.0\n",
    "test_mse_loss = 0.0\n",
    "test_mae_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for test_loader in test_loaders:\n",
    "        lst_loader_predictions = []    # each element of this list has predictions for one batch\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            # y_batch = y_batch.unsqueeze(0)  # Add batch dimension, to match the dimensionality of the model output.\n",
    "        \n",
    "            # predictions = model(X_batch)\n",
    "            # test_loss += loss_function(predictions, y_batch).item()\n",
    "\n",
    "            # # calculate additional metrics\n",
    "            # test_mse_loss += mse_loss_function(predictions, y_batch).item()\n",
    "            # test_mae_loss += mae_loss_function(predictions, y_batch).item()\n",
    "\n",
    "            if HYBRYD_MODEL or (use_physics ^ use_ml):\n",
    "                predictions = model.forward(X_batch)\n",
    "                test_loss += loss_function(predictions, y_batch).item()\n",
    "\n",
    "                # calculate additional metrics\n",
    "                test_mse_loss += mse_loss_function(predictions, y_batch).item()\n",
    "                test_mae_loss += mae_loss_function(predictions, y_batch).item()\n",
    "            else:\n",
    "                predictions = model.forward(X_batch)\n",
    "                prediction_ml = predictions[0]\n",
    "                prediction_ml = prediction_ml.unsqueeze(0)    # Add 1 dimension\n",
    "                prediction_physics = predictions[1]\n",
    "                prediction_physics = prediction_physics.unsqueeze(0)    # Add 1 dimension\n",
    "                test_loss += ( loss_function(prediction_ml, y_batch) + \\\n",
    "                            (lambda_ * loss_function(prediction_physics, prediction_ml)) ).item()\n",
    "\n",
    "                # calculate additional metrics\n",
    "                test_mse_loss += ( mse_loss_function(prediction_ml, y_batch) + \\\n",
    "                        (lambda_ * mse_loss_function(prediction_physics, prediction_ml)) ).item()\n",
    "                test_mae_loss += ( mae_loss_function(prediction_ml, y_batch) + \\\n",
    "                        (lambda_ * mae_loss_function(prediction_physics, prediction_ml)) ).item()\n",
    "                \n",
    "            # save predictions\n",
    "            if HYBRYD_MODEL or (use_physics ^ use_ml):\n",
    "                lst_loader_predictions.extend(predictions)    \n",
    "            else:\n",
    "                predictions_to_store = predictions[0].unsqueeze(0)    # Add 1 dimension\n",
    "                lst_loader_predictions.extend(predictions_to_store)\n",
    "    \n",
    "        # save predictions of the corresponding test_loader, since it has the prediction for one file of the dataset\n",
    "        lst_all_loader_predictions.append(lst_loader_predictions)\n",
    "\n",
    "# print main loss\n",
    "avg_test_loss = test_loss / n_test_batches\n",
    "print(f\"Test Loss (RMSE) - Metric used for training: {avg_test_loss:.6f}\")\n",
    "\n",
    "# print additional metrics\n",
    "avg_mse_loss = test_mse_loss / n_test_batches\n",
    "avg_mae_loss = test_mae_loss / n_test_batches\n",
    "print(f\"Test Loss (MSE): {avg_mse_loss:.6f}\")\n",
    "print(f\"Test Loss (MAE): {avg_mae_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21127da4-e293-419c-bbcf-c0af26a16268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lst_all_loader_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d86448-4a77-4db7-b41c-972d12dd598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model output to store it as additional column in test set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47aae87-78cb-4051-943b-ed02c348c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lst_all_loader_predictions into NumPy arrays\n",
    "\n",
    "# Stack the list of tensors into a single tensor\n",
    "lst_all_loader_predictions = [torch.stack(tensor_list).cpu() for tensor_list in lst_all_loader_predictions]\n",
    "\n",
    "\n",
    "# inverse_transform the scaling of the y_predicted values\n",
    "lst_all_loader_predictions = [scaler_y.inverse_transform(predictions) for predictions in lst_all_loader_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927da7f-22d4-4e38-ad99-6632f6c40123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lst_all_loader_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd7c54-85bf-48a6-a092-1af63d877a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save X_test, y_test and model predictions together in files\n",
    "\n",
    "files_data = []    # each dataloader has batched data of 1 file of the (test) dataset, which will be stored here. So each element of this list is 1 file.\n",
    "for idx_dataloader in range(len(test_loaders)):\n",
    "    file_data = []    # each element is one batch of the file\n",
    "    \n",
    "    for X_batch, y_batch in test_loaders[idx_dataloader]:\n",
    "        # combine X, y and  data to have everything in one list\n",
    "        X_data = scaler_X.inverse_transform(X_batch.cpu().numpy())\n",
    "        y_data = scaler_y.inverse_transform(y_batch.cpu().numpy())\n",
    "        X_and_y = [list(l1) + list(l2) for l1, l2 in zip(X_data, y_data)]\n",
    "    \n",
    "        file_data.extend(X_and_y)\n",
    "\n",
    "    file_data = np.array(file_data)\n",
    "    file_predictions_array = np.array(lst_all_loader_predictions[idx_dataloader])\n",
    "    file_data = [list(l1) + list(l2) for l1, l2 in zip(file_data, file_predictions_array)]\n",
    "        \n",
    "    files_data.append(file_data)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca285a-d5f1-4b4c-9a50-b96f25368042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cee23d-ebce-404f-b096-e2a398052fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ground truth vs model Estimation of one test file\n",
    "\n",
    "data = files_data[0]\n",
    "\n",
    "# Extracting the last and second last elements of each sublist\n",
    "last_elements = [row[-1] for row in data]\n",
    "second_last_elements = [row[-2] for row in data]\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(last_elements, label=\"Model estimation\", linestyle='dashed')\n",
    "plt.plot(second_last_elements, label=\"Ground Truth\")\n",
    "\n",
    "# Adding labels, title, and legend\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.title(\"Ground Truth vs Estimation\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1efc42-2839-4ab7-9c8a-58e1376593e7",
   "metadata": {},
   "source": [
    "## Store model and test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed5e8b-8479-483c-b9c8-4d36226c8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store each element of files_data as csv file\n",
    "\n",
    "# Create folder (if it doesn't exist) to store test predictions and X and y features\n",
    "output_folder_test_files = f\"{output_folder}/test_data_and_model_output\"\n",
    "os.makedirs(output_folder_test_files, exist_ok=True)\n",
    "\n",
    "# create column name for model output\n",
    "estimation_column_name = f\"\"\"{'Physics' if model.use_physics else ''}{'-and-' if model.use_physics and model.use_ml else ''}{'Transformer' if model.use_ml else ''}\"\"\"\n",
    "\n",
    "for idx_file_data in range(len(files_data)):\n",
    "    file_data = files_data[idx_file_data]\n",
    "\n",
    "    # create df\n",
    "    columns = list(X_column_names.copy())\n",
    "\n",
    "    columns.extend([\"SoC [%]\", estimation_column_name])\n",
    "    # columns = [\"Time [s]\", \"SoC_0\", \"Q_rated\", \"Battery Voltage [V]\", \"Battery Current [A]\", \"Time_difference\", \"SoC [%]\", \"Estimated SoC (Transformer model)\"]\n",
    "    df = pd.DataFrame(file_data, columns=columns)\n",
    "\n",
    "    # save in created folder\n",
    "    output_file = f\"file-{idx_file_data}.csv\"\n",
    "    output_file = os.path.join(output_folder_test_files,output_file)\n",
    "    df.to_csv(output_file, index=True, sep=\";\", encoding=\"ISO-8859-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88aac4f-cfc7-43ca-89d3-0b1ff1b67099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model with minimum evaluation loss\n",
    "torch.save(best_model_state, f\"{output_folder}/model-{model_name}-minimum_eval_loss-epoch-{best_model_epoch}.pth\")\n",
    "\n",
    "# Save scalers\n",
    "joblib.dump(scaler_X, f\"{output_folder}/X-scaler.pkl\")\n",
    "joblib.dump(scaler_y, f\"{output_folder}/y-scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fffb46-a710-4a0d-8fa4-db9477ba27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export hyperparameters and other relevant variables\n",
    "hyperparameters_and_other_relevant_variables = {\n",
    "    \"model_name\": model_name,\n",
    "    \"NORMALIZE_DATA\": NORMALIZE_DATA,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"ML_MODEL_USES_CALCULATED_COLUMNS\": ML_MODEL_USES_CALCULATED_COLUMNS,\n",
    "    \"dataset_folder\": dataset_folder,\n",
    "    \"required_columns\": required_columns,\n",
    "    \"calculated_columns\": calculated_columns,\n",
    "    \"device\": torch.cuda.get_device_name(device),\n",
    "    \"d_model\": d_model,\n",
    "    # \"num_heads\": num_heads,\n",
    "    # \"num_layers\": num_layers,\n",
    "    # \"dropout\": dropout,\n",
    "    # \"dim_feedforward\": dim_feedforward,\n",
    "    \"input_dim\": input_dim,\n",
    "    # \"output_dim\": output_dim,\n",
    "    \"use_physics\": use_physics,\n",
    "    \"use_ml\": use_ml,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"n_epochs\": n_epochs,\n",
    "    \"patience\": patience,\n",
    "    \"evaluattion_frequency_in_epochs\": evaluattion_frequency_in_epochs,\n",
    "    \"HYBRYD_MODEL\": HYBRYD_MODEL,\n",
    "}\n",
    "\n",
    "\n",
    "with open(f\"{output_folder}/hyperparameters_and_relevant_variables.json\", 'w') as file:\n",
    "    json.dump(hyperparameters_and_other_relevant_variables, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae7e89d-fc8a-42c8-adef-94ca3147c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export metrics\n",
    "metrics = {\n",
    "    \"best model - validation set RMSE\": all_validation_rmses[best_model_epoch],\n",
    "    \"test set loss (RMSE)\": avg_test_loss,\n",
    "    \"test set loss (MSE)\": avg_mse_loss,\n",
    "    \"test set loss (MAE)\": avg_mae_loss,\n",
    "}\n",
    "\n",
    "with open(f\"{output_folder}/metrics.json\", 'w') as file:\n",
    "    json.dump(metrics, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ebfcb1-d00e-41e6-abbb-183009a4984e",
   "metadata": {},
   "source": [
    "## Empty cuda cache if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda285f-ac29-4241-a7d0-c4e2ea182122",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cddae6-2be2-47a7-858b-dcd636da4b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
