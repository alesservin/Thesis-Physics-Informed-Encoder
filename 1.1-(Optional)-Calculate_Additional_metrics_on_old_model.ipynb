{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311acafb-eb97-4502-9029-5279116cd3af",
   "metadata": {},
   "source": [
    "Some models have been created before the addition of metrics like MAE and MSE. This notebook adds them to the metrics.json.\n",
    "\n",
    "How to use this notebook?\n",
    "\n",
    "Locate this notebook in the directory where the model, metrics.json and data folder are located (Example: locate this notebook in results\\1-For-Microtrips\\Physics-and-MachineLearning). Then, run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e0cae49-d921-423d-9093-70bdba9dccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c0241-0c5c-4f50-ad5f-8a1081bbf756",
   "metadata": {},
   "source": [
    "# Get test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b477c81-415a-41d5-829a-67de28556254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths_with_extension(file_extension=\".csv\", directory=\".\"):\n",
    "    output = []\n",
    "    \n",
    "    for file in os.listdir(directory):  \n",
    "        # Check if the file has the required extension\n",
    "        if file.endswith(file_extension):\n",
    "            output.append(f\"{directory}/{file}\") \n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628a8e64-d70c-4929-8442-6b95dd9a58ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_data_and_model_output/file-0.csv',\n",
       " 'test_data_and_model_output/file-1.csv',\n",
       " 'test_data_and_model_output/file-2.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_filepaths = get_filepaths_with_extension(\".csv\", \"test_data_and_model_output\")\n",
    "all_data_filepaths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268a39bd-e1a1-43f4-b890-ab35ede910b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all model results\n",
    "# each model will be represented by an index, and inside that index, all the dfs that contain the test results\n",
    "lst_dfs = []    \n",
    "\n",
    "for idx_df in range(len(all_data_filepaths)):\n",
    "    lst_dfs = [pd.read_csv(filepath, sep=\";\", encoding=\"ISO-8859-2\") for filepath in all_data_filepaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca58830a-880e-43ae-9f70-bc74218789c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc203e-e1f6-478c-842b-06771b89ede7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bee6c690-a767-4cae-bd0b-d4b1c6699bf8",
   "metadata": {},
   "source": [
    "# Calculate additional losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19a92527-d4d2-4583-a737-ea82d33bc82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(filename_start, file_extension=\".csv\", directory=\".\"):\n",
    "\n",
    "    for file in os.listdir(directory):  \n",
    "        # Check if the file has the required extension\n",
    "        if file.endswith(file_extension) and file.startswith(filename_start):\n",
    "            return f\"{directory}/{file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "671f8314-9998-4bb1-9aa2-469053810632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load used scaler for y and y_hat\n",
    "y_scaler_filename = get_file(\"y-scaler\", file_extension=\".pkl\")\n",
    "y_scaler = joblib.load(y_scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca3f116-d394-41ef-95ce-ab64a624bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def rmse_loss(y_true, y_pred):\n",
    "    mse_loss = torch.nn.functional.mse_loss(y_pred, y_true) \n",
    "    return torch.sqrt(mse_loss)  \n",
    "    \n",
    "# loss_function =  torch.nn.MSELoss()  \n",
    "loss_function = rmse_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb628f26-75a9-4033-a035-1aabbe7d10bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.Test Loss (RMSE) - Metric used for training: 0.129499\n",
      "2.Test Loss (MSE): 0.019651\n",
      "2.Test Loss (MAE): 0.113145\n"
     ]
    }
   ],
   "source": [
    "mse_loss_function = torch.nn.functional.mse_loss\n",
    "mae_loss_function = torch.nn.functional.l1_loss\n",
    "\n",
    "test_mse_loss = 0\n",
    "test_mae_loss = 0\n",
    "test_loss = 0\n",
    "\n",
    "for df in lst_dfs:\n",
    "    # get necessary data for calculation of metrics\n",
    "    ground_truth = df[\"SoC [%]\"].values\n",
    "    model_output_column_name = df.columns[-1]\n",
    "    predictions = df[model_output_column_name].values\n",
    "\n",
    "    # reshape predictions and ground truth \n",
    "    predictions = predictions.reshape(-1, 1)\n",
    "    ground_truth = ground_truth.reshape(-1, 1)\n",
    "\n",
    "    # scale predictions and ground truth as in original evaluation\n",
    "    predictions = y_scaler.transform(predictions)\n",
    "    ground_truth = y_scaler.transform(ground_truth)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    predictions = torch.tensor(predictions, dtype=torch.float32)\n",
    "    ground_truth = torch.tensor(ground_truth, dtype=torch.float32)\n",
    "    \n",
    "    # calculate additional metrics\n",
    "    test_loss += loss_function(predictions, ground_truth).item()\n",
    "    test_mse_loss += mse_loss_function(predictions, ground_truth).item()\n",
    "    test_mae_loss += mae_loss_function(predictions, ground_truth).item()\n",
    "\n",
    "# print main loss\n",
    "avg_test_loss = test_loss / len(lst_dfs)\n",
    "print(f\"2.Test Loss (RMSE) - Metric used for training: {avg_test_loss:.6f}\")\n",
    "\n",
    "# print additional metrics\n",
    "avg_mse_loss = test_mse_loss / len(lst_dfs)\n",
    "avg_mae_loss = test_mae_loss / len(lst_dfs)\n",
    "print(f\"2.Test Loss (MSE): {avg_mse_loss:.6f}\")\n",
    "print(f\"2.Test Loss (MAE): {avg_mae_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad64b9-98c1-4f19-89c2-06fe7c6fdfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "693574b7-e1bd-41b8-936c-b76b079fbd32",
   "metadata": {},
   "source": [
    "# Export metrics to existing file or create new file if no metrics.json found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c72825-ddef-4df3-89c1-109a18fc197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Load a JSON file and return its contents as a dictionary.\n",
    "    :param file_path: Path to the JSON file.\n",
    "    :return: Dictionary containing JSON data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{file_path}' was not found. Returning empty dictionary.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aa7bd37-41d3-48c8-987b-bfd158110615",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"metrics.json\"\n",
    "metrics = load_json_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca73eb31-8fe7-413c-9fbe-97ea2c5e0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add calculated metrics to imported metrics dictionary\n",
    "metrics[\"2.test set loss (RMSE) - used for training\"] = avg_test_loss\n",
    "metrics[\"2.test set loss (MSE)\"] = avg_mse_loss\n",
    "metrics[\"2.test set loss (MAE)\"] = avg_mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "751c71f7-baab-4144-8cf3-3da5ca05f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export metrics\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(metrics, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db2e10-d4cd-48c4-8423-4122529d8561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
